# Dcard æ–‡ç« æ„›å¿ƒæ•¸é æ¸¬æ¼”ç®—æ³•
é€™å€‹Projectçš„ç›®çš„æ˜¯ï¼Œåœ¨çµ¦äºˆæ–‡ç« ç™¼ä½ˆå¾Œå‰6å°æ™‚çš„ç‹€æ…‹åŠåŸºæœ¬è³‡è¨Šï¼Œé æ¸¬ 24 å°æ™‚å¾Œçš„æ„›å¿ƒæ•¸ã€‚å°‡ä½¿ç”¨æ©Ÿå™¨å­¸ç¿’çš„æ–¹æ³•åšå›æ­¸é æ¸¬ã€‚

**æ–‡ç« ç‹€æ…‹èˆ‡åŸºæœ¬è³‡è¨Š**
* `title`: æ–‡ç« æ¨™é¡Œ
* `created_at`: æ–‡ç« ç™¼ä½ˆæ™‚é–“
* `like_count_1~6h`: æ–‡ç« ç™¼ä½ˆå¾Œ 1~6 å°æ™‚çš„ç´¯ç©æ„›å¿ƒæ•¸
* `comment_count_1~6h`: æ–‡ç« ç™¼ä½ˆå¾Œ 1~6 å°æ™‚çš„ç´¯ç©ç•™è¨€æ•¸
* `forum_id`: æ–‡ç« ç™¼ä½ˆçœ‹æ¿ ID
* `author_id`: æ–‡ç« ä½œè€… ID
* `forum_stats`: çœ‹æ¿è³‡è¨Š
## ç’°å¢ƒéœ€æ±‚
* å®‰è£Python3çš„è™›æ“¬ç’°å¢ƒã€‚
* å®‰è£Pytorchå¾Œç«¯æ¡†æ¶ï¼Œå°æ–¼è©²å¦‚ä½•åœ¨ä½ ä½¿ç”¨çš„å¹³å°ä¸Šå®‰è£é€™äº›æ¡†æ¶ï¼Œå¯ä»¥åƒè€ƒPyTorch[å®‰è£é é¢](https://pytorch.org/get-started/locally/#start-locally)ã€‚
* åœ¨è™›æ“¬ç’°å¢ƒä¸‹å®‰è£ğŸ¤—Transformersã€‚
``` python
pip install transformers
```
## æ–¹æ³•
é€™å€‹Projectä½¿ç”¨äº†4ç¨®æ©Ÿå™¨å­¸ç¿’æ¼”ç®—æ³•ï¼Œåˆ†åˆ¥æ˜¯ï¼š
* Linear Regression in `ML.py`
* Logistic Regression in `ML.py`
* Support Vector Regression (SVR) in `ML.py`
* NLP Regression Model With ğŸ¤—Transformers in `NLP.py`
## è³‡æ–™å‰è™•ç†
å°‡éœ€è¦çš„è³‡æ–™ä¸‹è¼‰ä¸‹ä¾†ä¸¦å­˜ç‚º`train`ï¼Œ`valid`å’Œ`test`ï¼Œè³‡æ–™åˆ†åˆ¥æ˜¯5000ç­†ï¼Œ10000ç­†åŠ10000ç­†ã€‚
``` python
import pandas as pd
train = pd.read_csv("./intern_homework_train_dataset.csv")
valid = pd.read_csv("./intern_homework_public_test_dataset.csv")
test = pd.read_csv("./intern_homework_private_test_dataset.csv")
```
æª¢æŸ¥è³‡æ–™æ˜¯å¦æœ‰ç©ºå€¼ï¼Œç™¼ç¾3å€‹è³‡æ–™éƒ½æ²’æœ‰ç©ºå€¼ã€‚
``` python
print("train data\n", train.isnull().sum())
print("validation data\n",valid.isnull().sum())
print("test data\n", test.isnull().sum())
```
* train data

|colums     |number of null|
|-----------|:------------:|
|title           |0|
|created_at      |0|
|like_count_1h   |0|
|like_count_2h   |0|
|like_count_3h   |0|
|like_count_4h   |0|
|like_count_5h   |0|
|like_count_6h   |0|
|comment_count_1h|0|
|comment_count_2h|0|
|comment_count_3h|0|
|comment_count_4h|0|
|comment_count_5h|0|
|comment_count_6h|0|
|forum_id        |0|
|author_id       |0|
|forum_stats     |0|
|like_count_24h  |0|
* validation data

|colums     |number of null|
|-----------|:------------:|
|title           |0|
|created_at      |0|
|like_count_1h   |0|
|like_count_2h   |0|
|like_count_3h   |0|
|like_count_4h   |0|
|like_count_5h   |0|
|like_count_6h   |0|
|comment_count_1h|0|
|comment_count_2h|0|
|comment_count_3h|0|
|comment_count_4h|0|
|comment_count_5h|0|
|comment_count_6h|0|
|forum_id        |0|
|author_id       |0|
|forum_stats     |0|
|like_count_24h  |0|
* test data

|colums     |number of null|
|-----------|:------------:|
|title           |0|
|created_at      |0|
|like_count_1h   |0|
|like_count_2h   |0|
|like_count_3h   |0|
|like_count_4h   |0|
|like_count_5h   |0|
|like_count_6h   |0|
|comment_count_1h|0|
|comment_count_2h|0|
|comment_count_3h|0|
|comment_count_4h|0|
|comment_count_5h|0|
|comment_count_6h|0|
|forum_id        |0|
|author_id       |0|
|forum_stats     |0|
|like_count_24h  |0|
æ¥ä¸‹ä¾†è¦æª¢æŸ¥çš„æ˜¯ï¼Œdataè£¡é¢ä¸åŒç¨®é¡çš„å€¼æœ‰å¤šå°‘å€‹ï¼Œé€™é‚Šç”¨train dataä¾†èˆ‰ä¾‹ã€‚
``` python
train.nunique()
```
å¯¦ä½œå¾Œæœƒç™¼ç¾åŸºæœ¬ä¸Šæ²’æœ‰ç‰¹æ®Šçš„æŸäº›é¡åˆ¥ã€‚
|colums     |number of value|
|-----------|:------------:|
|title           |49158|
|created_at      |49654|
|like_count_1h   |112|
|like_count_2h   |185|
|like_count_3h   |250|
|like_count_4h   |308|
|like_count_5h   |380|
|like_count_6h   |438|
|comment_count_1h|172|
|comment_count_2h|216|
|comment_count_3h|252|
|comment_count_4h|277|
|comment_count_5h|305|
|comment_count_6h|331|
|forum_id        |1147|
|author_id       |32280|
|forum_stats     |221|
|like_count_24h  |940|
å°‡ç›®æ¨™çš„`like_count_24`èˆ‡å…¶ä»–æ¬„ä½çš„ç›¸é—œæ€§å°å‡ºä¾†ï¼Œç™¼ç¾ç›¸é—œæ€§ç›¸å°é«˜çš„æ˜¯`like_count_5h`å’Œ`like_count_6h`ã€‚
``` python
import matplotlib.pyplot as plt
import seaborn as sns

corrmat = train.corr()['like_count_24h']
corrmat
```
|colums     |correlation|
|-----------|:------------:|
|like_count_1h   |0.398345|
|like_count_2h   |0.467885|
|like_count_3h   |0.559062|
|like_count_4h   |0.648533|
|like_count_5h   |0.713661|
|like_count_6h   |0.760585|
|comment_count_1h|0.037055|
|comment_count_2h|0.048422|
|comment_count_3h|0.061782|
|comment_count_4h|0.074955|
|comment_count_5h|0.086810|
|comment_count_6h|0.094525|
|forum_id        |0.028790|
|author_id       |-0.004098|
|forum_stats     |0.045711|
|like_count_24h  |1.000000|
å‰è™•ç†çš„æœ€å¾Œï¼Œå‰”é™¤ä¸è¦çš„æ¬„ä½ï¼Œåšå‡ºè¦æ”¾é€²æ¨¡å‹çš„dataã€‚
``` python
train.drop(columns={'title','created_at','forum_id','author_id','forum_stats'}, inplace=True)
valid.drop(columns={'title','created_at','forum_id','author_id','forum_stats'}, inplace=True)
```
### æ¼”ç®—æ³•
åœ¨è¨“ç·´å„ç¨®æ¨¡å‹ä¹‹å‰ï¼Œé¦–å…ˆï¼Œå…ˆå°‡è³‡æ–™æ•´ç†æˆtrainingå’Œvalidationå…©å€‹éƒ¨åˆ†ã€‚
``` python
import numpy as np
X_train = np.array(train.iloc[:,:-1])
X_valid = np.array(valid.iloc[:,:-1])
y_train = np.array(train['like_count_24h'].tolist())
y_valid = np.array(valid['like_count_24h'].tolist())
```
* Linear Regression
``` python
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_percentage_error

model = LinearRegression()
model.fit(X_train , y_train)
#Predictions on Testing data
y_pred = model.predict(X_valid) 
# around() Evenly round to the given number of decimals.
r2 = r2_score(y_valid, np.around(y_pred,0))
mse = mean_squared_error(y_valid, np.around(y_pred,0))
mape = mean_absolute_percentage_error(y_valid, np.around(y_pred,0))*100

print('MSE:', mse)
print('R2 Score:', r2)
print('MAPE:', mape)
```
MSE: 13471.8647
R2 Score: 0.5194420923678873
MAPE: 87.97600954035927
* Logistic Regression`filename.py`
``` python
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_percentage_error
sc_X = StandardScaler()
X_train = sc_X.fit_transform(X_train)
X_valid = sc_X.transform(X_valid)
model_log = LogisticRegression()
model_log.fit(X_train , y_train)
# Predictions on Testing data
y_pred = model_log.predict(X_valid) 

r2 = r2_score(y_valid, np.around(y_pred,0))
mse = mean_squared_error(y_valid, np.around(y_pred,0))
mape = mean_absolute_percentage_error(y_valid, np.around(y_pred,0))*100

print('MSE:', mse)
print('R2 Score:', r2)
print('MAPE:', mape)
```
MSE: 24845.2405
R2 Score: 0.11373985300664236
MAPE: 40.81011815359801
* Support Vector Regressionï¼ˆSVRï¼‰`filename.py`
``` python
import numpy as np
from sklearn.svm import SVR
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_percentage_error
```
`SVR(kernel='rbf',C=1)`
``` python
# SVR C=1
sc_X = StandardScaler()
X_train = sc_X.fit_transform(X_train)
X_valid = sc_X.transform(X_valid)
svr = SVR(kernel='rbf')
svr.fit(X_train, y_train)
#Predictions on Testing data
y_pred = svr.predict(X_valid)
r2 = r2_score(y_valid, y_pred)
mse = mean_squared_error(y_valid, y_pred)
mape = mean_absolute_percentage_error(y_valid, np.around(y_pred,0))*100

print('MSE:', mse)
print('R2 Score:', r2)
print('MAPE:', mape)
```
MSE: 23013.16228320222
R2 Score: 0.17909232603754466
MAPE: 36.62957492416708
`SVR(kernel='rbf',C=1e3)`
``` python
# SVR C=1e3
sc_X = StandardScaler()
X_train = sc_X.fit_transform(X_train)
X_valid = sc_X.transform(X_valid)
svr = SVR(kernel='rbf',C=1e3)
svr.fit(X_train, y_train)
#Predictions on Testing data
y_pred = svr.predict(X_valid)
r2 = r2_score(y_valid, y_pred)
mse = mean_squared_error(y_valid, y_pred)
mape = mean_absolute_percentage_error(y_valid, np.around(y_pred,0))*100

print('MSE:', mse)
print('R2 Score:', r2)
print('MAPE:', mape)
```
MSE: 15019.091487019512
R2 Score: 0.4642506185845696
MAPE: 34.02267396110232
* NLP Regression Model With ğŸ¤—Transformers`filename.py`
NLP Regression Model ä½¿ç”¨çš„æ˜¯Hugging faceğŸ¤—çš„é è¨“ç·´æ¨¡å‹ã€‚é¦–å…ˆï¼Œå…ˆ`import`ç›¸é—œçš„libraryã€‚
``` python
import numpy as np
import pandas as pd
import transformers
from datasets import Dataset, DatasetDict, load_dataset, load_from_disk
from transformers import AutoTokenizer, AutoModelForSequenceClassification
```
å› ç‚ºé€™æ˜¯ä¸€å€‹ä½¿ç”¨è‡ªç„¶èªè¨€è™•è£¡åšå›æ­¸é æ¸¬çš„æ¨¡å‹ï¼Œæ‰€ä»¥æˆ‘å€‘åªç”¨`title`ä¾†ç•¶ä½œæˆ‘å€‘çš„ç‰¹å¾µã€‚
``` python
train_set = train[['title', 'like_count_24h']]
train_set = train_set.rename({'like_count_24h': 'labels'}, axis=1)
valid_set = valid[['title', 'like_count_24h']]
valid_set = valid_set.rename({'like_count_24h': 'labels'}, axis=1)
```
å°‡dataè½‰æ›æˆ`DatasetDict`
``` python
dataset = DatasetDict({"train": Dataset.from_pandas(train_set),
                      "valid": Dataset.from_pandas(valid_set)})
dataset
```
Output
```
DatasetDict({
train: Dataset({
        features: ['title', 'labels'],
        num_rows: 50000
    })
    valid: Dataset({
        features: ['title', 'labels'],
        num_rows: 10000
    })
})
```
æŠŠdatasetæº–å‚™å¥½ï¼Œå°±å¯ä»¥ä¾†å»ºæ§‹æ¨¡å‹äº†ï¼æˆ‘å€‘ç”¨çš„Tokenizerå’ŒModeléƒ½æ˜¯ğŸ¤—çš„[bert-base-chinese](https://huggingface.co/bert-base-chinese#model-details)é è¨“ç·´æ¨¡å‹ï¼Œé€™å€‹æ¨¡å‹å·²ç¶“é‡å°ä¸­æ–‡é€²è¡Œäº†é è¨“ç·´ï¼Œè©³æƒ…å¯ä»¥åƒè€ƒåŸå§‹[BERT](https://arxiv.org/abs/1810.04805)è«–æ–‡ã€‚
``` pytohn 
BASE_MODEL = "bert-base-chinese"
LEARNING_RATE = 2e-5
# MAX_LENGTH = 256
BATCH_SIZE = 4
EPOCHS = 20

tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)
# set num_labels=1 -> linear regression model
model = AutoModelForSequenceClassification.from_pretrained(BASE_MODEL, num_labels=1)
```
æ¥è‘—ï¼Œå°‡è³‡æ–™è®Šæˆä¸€å€‹ä¸€å€‹çš„Tokenï¼Œæ‰€ä»¥æˆ‘å€‘æŠŠè³‡æ–™çš„`title`ä¸Ÿåˆ°Tokenizerè£¡é¢ã€‚
å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œé™¤äº†åŸæœ¬çš„`title`å’Œ`labels`ï¼Œé‚„æœ‰3å€‹featureï¼Œ`input_ids`ï¼Œ`token_type_ids`ä»¥åŠ`attention_mask`ï¼Œé€™3å€‹å‰‡æ˜¯æ¨¡å‹åœ¨åšSelf-attentionæ™‚éœ€è¦çš„å‘é‡ã€‚
``` python
def tokenize_function(examples):
    return tokenizer(examples["title"], padding="max_length", truncation=True)

tokenized_datasets = dataset.map(tokenize_function, batched=True)
tokenized_datasets
```
Output
```
DatasetDict({
    train: Dataset({
        features: ['title', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],
        num_rows: 50000
    })
    valid: Dataset({
        features: ['title', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],
        num_rows: 10000
    })
})
```
åœ¨å›æ­¸çš„å•é¡Œä¸­ï¼Œæˆ‘å€‘æ˜¯è¦é æ¸¬ä¸€å€‹é€£çºŒå€¼ï¼Œéœ€è¦è¡¡é‡é æ¸¬å€¼èˆ‡çœŸå¯¦å€¼ä¹‹é–“è·é›¢çš„æŒ‡æ¨™ã€‚æ‰€ä»¥ï¼Œæˆ‘å€‘å‰µå»ºä¸€å€‹å‡½æ•¸`compute_metrics_for_regression`ï¼Œä»¥åœ¨è¨“ç·´æ•¸æ“šæ™‚ä½¿ç”¨å®ƒã€‚
``` python
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import mean_absolute_percentage_error
from sklearn.metrics import mean_squared_error
from sklearn.metrics import r2_score

def compute_metrics_for_regression(eval_pred):
    predictions, labels = eval_pred
    labels = labels.reshape(-1, 1)
    
    mse = mean_squared_error(labels, predictions)
    mae = mean_absolute_error(labels, predictions)
    mape = mean_absolute_percentage_error(labels, predictions)
    r2 = r2_score(labels, predictions)
    single_squared_errors = ((predictions - labels).flatten()**2).tolist()
    
    # Compute accuracy 
    # Based on the fact that the rounded score = true score only if |single_squared_errors| < 0.5
    accuracy = sum([1 for e in single_squared_errors if e < 0.25]) / len(single_squared_errors)
    
    return {"mse": mse, "mae": mae,"mape": mape, "r2": r2, "accuracy": accuracy}
```
è€Œè¨“ç·´æ™‚æˆ‘å€‘éœ€è¦æå¤±å‡½æ•¸ä¾†è©•ä¼°æ¨¡å‹æ˜¯å¦æœ‰æ”¶æ–‚ï¼Œé€™é‚Šçš„æå¤±å‡½æ•¸ä½¿ç”¨çš„æ˜¯Mean Square Error (MSE)ã€‚
``` python
import torch
class RegressionTrainer(Trainer):
    def compute_loss(self, model, inputs, return_outputs=False):
        labels = inputs.pop("labels")
        outputs = model(**inputs)
        logits = outputs[0][:, 0]
        loss = torch.nn.functional.mse_loss(logits, labels)
        return (loss, outputs) if return_outputs else loss
```
æœ€å¾Œï¼Œä½¿ç”¨Transformerså…§å»ºçš„TrainingArgumentså’ŒTrainerä¾†è¨“ç·´æ¨¡å‹ã€‚
``` python
from transformers import TrainingArguments, Trainer

training_args = TrainingArguments(
    output_dir="test_trainer_medium",
    learning_rate=LEARNING_RATE,
    per_device_train_batch_size=BATCH_SIZE,
    per_device_eval_batch_size=BATCH_SIZE,
    num_train_epochs=EPOCHS,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    save_total_limit=2,
    metric_for_best_model="accuracy",
    load_best_model_at_end=True,
    weight_decay=0.01,
)

trainer = RegressionTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["valid"],
    compute_metrics=compute_metrics_for_regression,
)
trainer.train()
```
æœ€å¥½çš„çµæœæ˜¯MAPE=134.7ã€‚
### çµæœ
åœ¨è¨“ç·´å®Œä»¥ä¸Šçš„æ¨¡å‹å¾Œé¢ï¼Œæœ€å¥½çš„çµæœæ˜¯ç”±`SVR(kernel='rbf',C=1e3)`è¨“ç·´å‡ºçš„æ¨¡å‹(MAPE= 34.02)ã€‚æ‰€ä»¥ï¼ŒæŠŠå‰é¢å‰è™•ç†å¥½çš„`test`ä¸Ÿåˆ°æ¨¡å‹è£¡é¢åšé æ¸¬ï¼Œä¸¦ç”¢å‡ºæœ€å¾Œçš„çµæœï¼Œå­˜æˆ`result.csv`ï¼Œå°±å¯ä»¥ç¹³äº¤äº†ï¼
``` python
test.drop(columns={'title','created_at','forum_id','author_id','forum_stats'}, inplace=True)
result_test = np.array(test)
result_pred = svr.predict(result_test)
result_pred = pd.DataFrame(np.around(result_pred,0).astype(int), columns=['like_count_24h'])
result_pred.to_csv("./result.csv") 
